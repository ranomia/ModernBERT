"""
JCommonsenseQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from typing import List, Dict, Any, Tuple
from datasets import load_dataset
import os
import shutil
import random


class JCommonsenseQADataset(Dataset):
    """JCommonsenseQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®Datasetã‚¯ãƒ©ã‚¹"""

    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int = 512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # è³ªå•ã¨é¸æŠè‚¢ã‚’çµåˆ
        question = item["question"]

        # HuggingFace Datasetså½¢å¼ã®é¸æŠè‚¢ã‚’å‡¦ç†
        choice_texts = []
        for i in range(5):  # choice0 ã‹ã‚‰ choice4 ã¾ã§
            choice_key = f"choice{i}"
            if choice_key in item:
                choice_text = f"{question} {item[choice_key]}"
                choice_texts.append(choice_text)
            else:
                choice_texts.append("")

        # æ­£è§£ãƒ©ãƒ™ãƒ«
        label = int(item["label"])

        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        encoded_choices = []
        for text in choice_texts:
            if text:  # ç©ºã§ãªã„å ´åˆã®ã¿
                encoded = self.tokenizer(
                    text,
                    max_length=self.max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt",
                )
                encoded_choices.append(
                    {
                        "input_ids": encoded["input_ids"].squeeze(),
                        "attention_mask": encoded["attention_mask"].squeeze(),
                    }
                )
            else:
                # ç©ºã®å ´åˆã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã§åŸ‹ã‚ã‚‹
                encoded_choices.append(
                    {
                        "input_ids": torch.zeros(self.max_length, dtype=torch.long),
                        "attention_mask": torch.zeros(
                            self.max_length, dtype=torch.long
                        ),
                    }
                )

        return {
            "choices": encoded_choices,
            "label": torch.tensor(label, dtype=torch.long),
            "question_id": item.get("q_id", idx),
        }


class JCommonsenseQALoader:
    """JCommonsenseQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ã‚¯ãƒ©ã‚¹"""

    def __init__(self, tokenizer_name: str = "cl-tohoku/bert-base-japanese-v3"):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    def clear_cache(self):
        """HuggingFace Datasetsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
        try:
            import datasets

            datasets.disable_caching()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–
            print("Disabled datasets caching for this session")

            # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ã‚¯ãƒªã‚¢
            cache_dir = os.path.expanduser("~/.cache/huggingface/datasets")
            if os.path.exists(cache_dir):
                print(f"Clearing datasets cache at: {cache_dir}")
                shutil.rmtree(cache_dir)
                print("Cache cleared successfully")
        except Exception as e:
            print(f"Warning: Could not clear cache: {e}")

    def load_data(self, split: str = "validation") -> List[Dict[str, Any]]:
        """
        HuggingFace Datasetsã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€

        Args:
            split: ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰² ("train" ã¾ãŸã¯ "validation")

        Returns:
            ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        # Colabç’°å¢ƒã§ã®äº’æ›æ€§è¨­å®š
        import os

        os.environ["HF_DATASETS_OFFLINE"] = "0"
        os.environ["TRANSFORMERS_OFFLINE"] = "0"
        os.environ["HF_DATASETS_TRUST_REMOTE_CODE"] = "1"

        try:
            # Colabã§ã®äº’æ›æ€§ã®ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–
            import datasets

            datasets.disable_caching()

            print(
                f"Loading JCommonsenseQA dataset from HuggingFace (split: {split})..."
            )

            # ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã‚ãªã„ï¼‰
            dataset_dict = load_dataset(
                "leemeng/jcommonsenseqa-v1.1"
            )

            print(f"Available splits: {list(dataset_dict.keys())}")
            
            # æŒ‡å®šã•ã‚ŒãŸsplitã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if split in dataset_dict:
                dataset = dataset_dict[split]
                print(f"âœ… Successfully found '{split}' split with {len(dataset)} samples")
            else:
                print(f"Warning: Split '{split}' not found. Available splits: {list(dataset_dict.keys())}")
                # validationãŒæŒ‡å®šã•ã‚Œã¦ã„ã¦testãŒã‚ã‚‹å ´åˆã¯testã‚’ä½¿ç”¨
                if split == "validation" and "test" in dataset_dict:
                    print("Using 'test' split instead of 'validation'")
                    dataset = dataset_dict["test"]
                elif split == "train" and "train" in dataset_dict:
                    print("Using 'train' split")
                    dataset = dataset_dict["train"]
                else:
                    # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸsplitã‚’ä½¿ç”¨
                    available_split = list(dataset_dict.keys())[0]
                    print(f"Using '{available_split}' split instead")
                    dataset = dataset_dict[available_split]

            # ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
            data = []
            for item in dataset:
                data.append(item)

            print(f"Successfully loaded {len(data)} samples from {split} split")
            
            # ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’ç°¡å˜ã«ç¢ºèª
            if len(data) > 0:
                first_item = data[0]
                print(f"Sample data structure: {list(first_item.keys())}")
                
            return data

        except Exception as e:
            print(f"âŒ Primary loading method failed: {e}")
            print("Trying alternative approaches...")

            # ä»£æ›¿æ–¹æ³•1: åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã§è©¦è¡Œ
            try:
                print("Trying alternative dataset: shunk031/JGLUE...")
                import datasets
                datasets.disable_caching()

                dataset = load_dataset(
                    "shunk031/JGLUE",
                    name="JCommonsenseQA",
                    split=split,
                    trust_remote_code=True,
                    verification_mode="no_checks",
                )

                data = []
                for item in dataset:
                    data.append(item)

                print(f"âœ… Successfully loaded {len(data)} samples using alternative dataset")
                return data

            except Exception as e2:
                print(f"âŒ Alternative dataset loading failed: {e2}")
                print("âš ï¸  All data loading methods failed. Creating dummy data for testing...")
                return self._create_dummy_data(split)

    def _create_dummy_data(self, split: str) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        dummy_data = []
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªã‚µãƒ³ãƒ—ãƒ«æ•°ã«å¢—åŠ 
        if split == "train":
            num_samples = 1000  # å­¦ç¿’ç”¨ã«ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«
        elif split == "validation":
            num_samples = 200   # æ¤œè¨¼ç”¨ã‚‚ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°
        else:
            num_samples = 100

        for i in range(num_samples):
            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆï¼ˆè¦å‰‡çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ’é™¤ï¼‰
            random_label = random.randint(0, 4)
            
            dummy_data.append(
                {
                    "q_id": i,
                    "question": f"ãƒ†ã‚¹ãƒˆè³ªå•{i}ï¼šã“ã‚Œã¯æ©Ÿæ¢°å­¦ç¿’ã®ãƒ†ã‚¹ãƒˆç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚",
                    "choice0": f"é¸æŠè‚¢A_{i}",
                    "choice1": f"é¸æŠè‚¢B_{i}",
                    "choice2": f"é¸æŠè‚¢C_{i}",
                    "choice3": f"é¸æŠè‚¢D_{i}",
                    "choice4": f"é¸æŠè‚¢E_{i}",
                    "label": random_label,  # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ©ãƒ™ãƒ«
                }
            )

        print(f"Created {len(dummy_data)} dummy samples for {split} split")
        print(f"âš ï¸  æ³¨æ„ï¼šã“ã‚Œã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¦ã„ã¾ã™ã€‚")
        return dummy_data

    def create_train_val_split(self, train_data: List[Dict[str, Any]], validation_split: float = 0.2) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        trainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²ã™ã‚‹
        
        Args:
            train_data: å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿
            validation_split: æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ (0.0-1.0)
            
        Returns:
            (train_split, val_split): åˆ†å‰²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        import random
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        shuffled_data = train_data.copy()
        random.shuffle(shuffled_data)
        
        # åˆ†å‰²ç‚¹ã‚’è¨ˆç®—
        split_idx = int(len(shuffled_data) * (1 - validation_split))
        
        train_split = shuffled_data[:split_idx]
        val_split = shuffled_data[split_idx:]
        
        print(f"ğŸ“Š trainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†å‰²ã—ã¾ã—ãŸ:")
        print(f"   - å­¦ç¿’ç”¨: {len(train_split)} ã‚µãƒ³ãƒ—ãƒ«")
        print(f"   - æ¤œè¨¼ç”¨: {len(val_split)} ã‚µãƒ³ãƒ—ãƒ«")
        
        return train_split, val_split

    def create_dataloader(
        self,
        split: str = "validation",
        batch_size: int = 8,
        max_length: int = 512,
        shuffle: bool = False,
        validation_split: float = 0.2,
        use_train_split: bool = False,
    ) -> DataLoader:
        """
        DataLoaderã‚’ä½œæˆã™ã‚‹
        
        Args:
            split: ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰² ("train", "validation", "train_split", "val_split")
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            max_length: æœ€å¤§ç³»åˆ—é•·
            shuffle: ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã‹ã©ã†ã‹
            validation_split: trainãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œè¨¼ç”¨ã«åˆ†å‰²ã™ã‚‹å‰²åˆï¼ˆsplit="train_split" or "val_split"æ™‚ã®ã¿ä½¿ç”¨ï¼‰
            use_train_split: Trueã®å ´åˆã€trainãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ä½¿ç”¨
        """
        if split in ["train_split", "val_split"]:
            # trainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§åˆ†å‰²
            train_data = self.load_data("train")
            train_split, val_split = self.create_train_val_split(train_data, validation_split)
            
            if split == "train_split":
                data = train_split
                print(f"âœ… å­¦ç¿’ç”¨åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨: {len(data)} ã‚µãƒ³ãƒ—ãƒ«")
            else:  # val_split
                data = val_split
                print(f"âœ… æ¤œè¨¼ç”¨åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨: {len(data)} ã‚µãƒ³ãƒ—ãƒ«")
        else:
            # é€šå¸¸ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
            data = self.load_data(split)
            if split == "validation":
                print(f"âœ… æœ€çµ‚è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆvalidationï¼‰ã‚’ä½¿ç”¨: {len(data)} ã‚µãƒ³ãƒ—ãƒ«")
            elif split == "train":
                print(f"âœ… å®Œå…¨ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆtrainï¼‰ã‚’ä½¿ç”¨: {len(data)} ã‚µãƒ³ãƒ—ãƒ«")
        
        dataset = JCommonsenseQADataset(data, self.tokenizer, max_length)

        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=0,
            collate_fn=self.collate_fn,
        )

    @staticmethod
    def collate_fn(batch):
        """ãƒãƒƒãƒå‡¦ç†ç”¨ã®collateé–¢æ•°"""
        batch_choices = []
        batch_labels = []
        batch_ids = []

        for item in batch:
            batch_choices.append(item["choices"])
            batch_labels.append(item["label"])
            batch_ids.append(item["question_id"])

        return {
            "choices": batch_choices,
            "labels": torch.stack(batch_labels),
            "question_ids": batch_ids,
        }


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    print("JCommonsenseQA Data Loader Test")

    # å¿…è¦ã«å¿œã˜ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ï¼‰
    # loader = JCommonsenseQALoader()
    # loader.clear_cache()

    loader = JCommonsenseQALoader()
    dataloader = loader.create_dataloader(split="validation", batch_size=2)

    for batch in dataloader:
        print(f"Batch size: {len(batch['choices'])}")
        print(f"Labels: {batch['labels']}")
        print(f"Question IDs: {batch['question_ids']}")
        break
