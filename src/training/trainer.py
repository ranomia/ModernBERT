"""
ç°¡å˜ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import sys
import os

# ãƒ‘ã‚¹ã‚’è¿½åŠ ã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.data_loader import JCommonsenseQALoader


class SimpleTrainer:
    """ç°¡å˜ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""

    def __init__(self, model, device, learning_rate=2e-5, use_mixed_precision=False):
        self.model = model
        self.device = device
        self.learning_rate = learning_rate
        self.use_mixed_precision = use_mixed_precision
        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()

        # æ··åˆç²¾åº¦å­¦ç¿’ç”¨ã®GradScaler
        if use_mixed_precision and device.type == "cuda":
            self.scaler = GradScaler()
            print("âœ… æ··åˆç²¾åº¦å­¦ç¿’ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã—ãŸ")
        else:
            self.scaler = None
            if use_mixed_precision and device.type != "cuda":
                print("âš ï¸  æ··åˆç²¾åº¦å­¦ç¿’ã¯CUDAç’°å¢ƒã§ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã—ãŸã€‚")

    def train_epoch(self, dataloader, epoch):
        """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        for batch_idx, batch in enumerate(
            tqdm(dataloader, desc=f"Training Epoch {epoch}")
        ):
            self.optimizer.zero_grad()

            # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
            batch_size = len(batch["choices"])
            input_ids_list = []
            attention_mask_list = []

            for choices in batch["choices"]:
                choice_input_ids = []
                choice_attention_mask = []

                for choice in choices:
                    choice_input_ids.append(choice["input_ids"])
                    choice_attention_mask.append(choice["attention_mask"])

                input_ids_list.append(torch.stack(choice_input_ids))
                attention_mask_list.append(torch.stack(choice_attention_mask))

            # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            input_ids = torch.stack(input_ids_list).to(self.device)
            attention_mask = torch.stack(attention_mask_list).to(self.device)
            labels = batch["labels"].to(self.device)

            # æ··åˆç²¾åº¦å­¦ç¿’å¯¾å¿œã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            if self.scaler is not None:
                # æ··åˆç²¾åº¦å­¦ç¿’ä½¿ç”¨
                with autocast():
                    logits = self.model(input_ids, attention_mask)
                    loss = self.criterion(logits, labels)

                # ãƒãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ··åˆç²¾åº¦ï¼‰
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # é€šå¸¸ã®å­¦ç¿’
                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)

                # ãƒãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰
                loss.backward()
                self.optimizer.step()

            # çµ±è¨ˆæ›´æ–°
            total_loss += loss.item()
            _, predicted = torch.max(logits.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ¶é™ã®ãŸã‚ã€å‹¾é…ã‚’ã‚¯ãƒªã‚¢
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else None

        accuracy = 100 * correct / total
        avg_loss = total_loss / len(dataloader)

        return avg_loss, accuracy

    def evaluate_epoch(self, dataloader):
        """1ã‚¨ãƒãƒƒã‚¯ã®æ¤œè¨¼"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Validation"):
                # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
                batch_size = len(batch["choices"])
                input_ids_list = []
                attention_mask_list = []

                for choices in batch["choices"]:
                    choice_input_ids = []
                    choice_attention_mask = []

                    for choice in choices:
                        choice_input_ids.append(choice["input_ids"])
                        choice_attention_mask.append(choice["attention_mask"])

                    input_ids_list.append(torch.stack(choice_input_ids))
                    attention_mask_list.append(torch.stack(choice_attention_mask))

                # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                input_ids = torch.stack(input_ids_list).to(self.device)
                attention_mask = torch.stack(attention_mask_list).to(self.device)
                labels = batch["labels"].to(self.device)

                # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ï¼ˆå‹¾é…è¨ˆç®—ãªã—ï¼‰
                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)

                # çµ±è¨ˆæ›´æ–°
                total_loss += loss.item()
                _, predicted = torch.max(logits.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = 100 * correct / total
        avg_loss = total_loss / len(dataloader)

        return avg_loss, accuracy

    def quick_finetune(
        self, tokenizer_name, num_epochs=3, batch_size=4, max_length=128
    ):
        """ã‚¯ã‚¤ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        print(f"ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
        print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {num_epochs}, ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        print(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²æˆ¦ç•¥:")
        print(f"   - å­¦ç¿’ç”¨: trainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®80%")
        print(f"   - æ¤œè¨¼ç”¨: trainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®20%")
        print(f"   - æœ€çµ‚è©•ä¾¡ç”¨: validationãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœªä½¿ç”¨ã§ãƒ†ã‚¹ãƒˆç”¨ã«ä¿å­˜ï¼‰")

        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        try:
            data_loader_instance = JCommonsenseQALoader(tokenizer_name=tokenizer_name)

            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆtrainãƒ‡ãƒ¼ã‚¿ã®80%ã‚’ä½¿ç”¨ï¼‰
            print("ğŸ“š å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆtrain_splitï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            train_dataloader = data_loader_instance.create_dataloader(
                split="train_split",  # trainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å­¦ç¿’ç”¨åˆ†å‰²
                batch_size=batch_size,
                max_length=max_length,
                shuffle=True,
                validation_split=0.2,  # 20%ã‚’æ¤œè¨¼ç”¨ã«åˆ†å‰²
            )
            
            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ï¼ˆtrainãƒ‡ãƒ¼ã‚¿ã®20%ã‚’ä½¿ç”¨ï¼‰
            print("ğŸ“– æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆval_splitï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            val_dataloader = data_loader_instance.create_dataloader(
                split="val_split",  # trainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼ç”¨åˆ†å‰²
                batch_size=batch_size,
                max_length=max_length,
                shuffle=False,
                validation_split=0.2,  # 20%ã‚’æ¤œè¨¼ç”¨ã«åˆ†å‰²
            )
            
            print("âœ… validationãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æœ€çµ‚è©•ä¾¡ç”¨ã«æ¸©å­˜ã•ã‚Œã¾ã™")
            
        except Exception as e:
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ç¶šè¡Œã—ã¾ã™...")

            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            data_loader_instance = JCommonsenseQALoader(tokenizer_name=tokenizer_name)
            
            # å­¦ç¿’ç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            train_dataloader = data_loader_instance.create_dataloader(
                split="train",  # ãƒ€ãƒŸãƒ¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä½œæˆã•ã‚Œã‚‹
                batch_size=batch_size,
                max_length=max_length,
                shuffle=True,
            )
            
            # æ¤œè¨¼ç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            val_dataloader = data_loader_instance.create_dataloader(
                split="validation",  # ãƒ€ãƒŸãƒ¼ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒä½œæˆã•ã‚Œã‚‹
                batch_size=batch_size,
                max_length=max_length,
                shuffle=False,
            )

        self.model.to(self.device)

        # å„ã‚¨ãƒãƒƒã‚¯ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼ã‚’å®Ÿè¡Œ
        for epoch in range(1, num_epochs + 1):
            # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º
            avg_loss, train_accuracy = self.train_epoch(train_dataloader, epoch)
            
            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
            val_loss, val_accuracy = self.evaluate_epoch(val_dataloader)
            
            print(
                f"Epoch {epoch}/{num_epochs} - "
                f"Train Loss: {avg_loss:.4f}, Train Acc: {train_accuracy:.2f}% | "
                f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%"
            )

        print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return self.model


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    from models.bert_model import BERTModel

    device = torch.device("cpu")  # CPUç’°å¢ƒç”¨
    model = BERTModel()
    trainer = SimpleTrainer(model, device)

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆï¼‰
    trainer.quick_finetune(
        tokenizer_name="cl-tohoku/bert-base-japanese-v3",
        num_epochs=1,
        batch_size=2,
        max_length=128,
    )
