"""
ModernBERT vs BERT æ€§èƒ½æ¯”è¼ƒãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import sys
import os
import torch
from datetime import datetime

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from evaluation.evaluator import ModelEvaluator
from models.bert_model import BERTModel
from models.modern_bert_model import ModernBERTModel


def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ"""
    parser = argparse.ArgumentParser(
        description="ModernBERT vs BERT æ€§èƒ½æ¯”è¼ƒè©•ä¾¡",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--model",
        type=str,
        choices=["bert", "modern_bert", "both"],
        default="both",
        help="è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ« (bert: BERT ã®ã¿, modern_bert: ModernBERT ã®ã¿, both: ä¸¡æ–¹)",
    )

    parser.add_argument("--batch_size", type=int, default=8, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")

    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§ç³»åˆ—é•·")

    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ (cuda, cpu, ã¾ãŸã¯ None ã§è‡ªå‹•é¸æŠ)",
    )

    parser.add_argument(
        "--output_dir", type=str, default="results", help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )

    parser.add_argument(
        "--bert_model",
        type=str,
        default="cl-tohoku/bert-base-japanese-v3",
        help="ä½¿ç”¨ã™ã‚‹BERTãƒ¢ãƒ‡ãƒ«å",
    )

    parser.add_argument(
        "--modern_bert_model",
        type=str,
        default="sbintuitions/modernbert-ja-130m",
        help="ä½¿ç”¨ã™ã‚‹ModernBERTãƒ¢ãƒ‡ãƒ«å",
    )

    return parser.parse_args()


def print_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å‡ºåŠ›"""
    print("=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ===")
    print(f"Python version: {sys.version}")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    print()


def setup_models_config(args):
    """ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æº–å‚™"""
    models_config = []

    if args.model in ["bert", "both"]:
        models_config.append(
            {
                "name": "BERT (æ±åŒ—å¤§v3)",
                "class": BERTModel,
                "args": {"model_name": args.bert_model, "num_choices": 5},
            }
        )

    if args.model in ["modern_bert", "both"]:
        models_config.append(
            {
                "name": "ModernBERT (SB-Intuitions)",
                "class": ModernBERTModel,
                "args": {"model_name": args.modern_bert_model, "num_choices": 5},
            }
        )

    return models_config


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ModernBERT vs BERT æ€§èƒ½æ¯”è¼ƒè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print(f"å®Ÿè¡Œé–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # å¼•æ•°è§£æ
    args = parse_arguments()

    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å‡ºåŠ›
    print_system_info()

    # è¨­å®šæƒ…å ±å‡ºåŠ›
    print("=== å®Ÿè¡Œè¨­å®š ===")
    print(f"è©•ä¾¡ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}")
    print(f"æœ€å¤§ç³»åˆ—é•·: {args.max_length}")
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {args.device if args.device else 'è‡ªå‹•é¸æŠ'}")
    print(f"çµæœå‡ºåŠ›å…ˆ: {args.output_dir}")
    print(f"BERTãƒ¢ãƒ‡ãƒ«: {args.bert_model}")
    print(f"ModernBERTãƒ¢ãƒ‡ãƒ«: {args.modern_bert_model}")
    print()

    try:
        # è©•ä¾¡å™¨åˆæœŸåŒ–
        evaluator = ModelEvaluator(device=args.device)

        # ãƒ¢ãƒ‡ãƒ«è¨­å®šæº–å‚™
        models_config = setup_models_config(args)

        if not models_config:
            print("ã‚¨ãƒ©ãƒ¼: è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        # è©•ä¾¡å®Ÿè¡Œ
        print("=== è©•ä¾¡é–‹å§‹ ===")
        results = evaluator.compare_models(
            models_config=models_config,
            batch_size=args.batch_size,
            max_length=args.max_length,
        )

        # çµæœä¿å­˜
        if args.output_dir != "results":
            evaluator.save_comparison_results(results, args.output_dir)

        # æœ€çµ‚ã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 80)
        print("=== è©•ä¾¡å®Œäº†ã‚µãƒãƒªãƒ¼ ===")
        print(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"è©•ä¾¡å¯¾è±¡: JCommonsenseQA ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        print(f"è©•ä¾¡ãƒ¢ãƒ‡ãƒ«æ•°: {len(results)}")

        for model_name, result in results.items():
            print(f"\nğŸ“Š {model_name}:")
            print(f"  âœ… æ­£è§£ç‡: {result['accuracy_metrics']['accuracy']:.4f}")
            print(f"  ğŸ“ˆ F1ã‚¹ã‚³ã‚¢: {result['accuracy_metrics']['f1_macro']:.4f}")
            print(
                f"  âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result['performance_metrics']['throughput_samples_per_second']:.2f} ã‚µãƒ³ãƒ—ãƒ«/ç§’"
            )
            print(
                f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {result['performance_metrics']['avg_memory_usage_mb']:.1f} MB"
            )
            print(f"  ğŸ”¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {result['model_info']['total_parameters']:,}")
            print(f"  ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {result['model_info']['model_size_mb']:.1f} MB")

        print(f"\nğŸ“ è©³ç´°çµæœã¯ {args.output_dir} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        print("\nğŸ‰ è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")

    except KeyboardInterrupt:
        print("\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦è©•ä¾¡ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã«ã¤ã„ã¦ã¯ã€ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        raise


if __name__ == "__main__":
    main()
