"""
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ä»˜ãModernBERT vs BERT æ€§èƒ½æ¯”è¼ƒãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import sys
import os
import torch
import japanize_matplotlib
from datetime import datetime

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from evaluation.evaluator import ModelEvaluator
from models.bert_model import BERTModel
from models.modern_bert_model import ModernBERTModel
from training.trainer import SimpleTrainer


def get_device():
    """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•é¸æŠ"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸš€ CUDA GPUæ¤œå‡º: {torch.cuda.get_device_name(0)}")
        print(
            f"   GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        )
        return device
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Apple MPS GPUæ¤œå‡º")
        return device
    else:
        device = torch.device("cpu")
        print("âš ï¸  GPUæœªæ¤œå‡º - CPUã‚’ä½¿ç”¨ã—ã¾ã™")
        return device


def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ"""
    parser = argparse.ArgumentParser(
        description="ModernBERT vs BERT æ€§èƒ½æ¯”è¼ƒè©•ä¾¡ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä»˜ãï¼‰",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--model",
        type=str,
        choices=["bert", "modern_bert", "both"],
        default="both",
        help="è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
    )

    # GPUä½¿ç”¨æ™‚ã®æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã«å¤‰æ›´
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆGPUä½¿ç”¨æ™‚æ¨å¥¨ï¼š16-32ï¼‰",
    )
    parser.add_argument("--max_length", type=int, default=128, help="æœ€å¤§ç³»åˆ—é•·")
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cuda", "mps", "cpu"],
        help="ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆautoã§è‡ªå‹•é¸æŠï¼‰",
    )

    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    parser.add_argument(
        "--finetune", action="store_true", help="ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"
    )
    parser.add_argument(
        "--epochs", type=int, default=3, help="ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯æ•°"
    )
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="å­¦ç¿’ç‡")

    # GPUæœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--mixed_precision", action="store_true", help="æ··åˆç²¾åº¦å­¦ç¿’ã‚’ä½¿ç”¨ï¼ˆGPUæ¨å¥¨ï¼‰"
    )

    return parser.parse_args()


def setup_gpu_optimizations(device):
    """GPUä½¿ç”¨æ™‚ã®æœ€é©åŒ–è¨­å®š"""
    if device.type == "cuda":
        # cuDNNæœ€é©åŒ–
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        torch.cuda.empty_cache()

        print("âœ… GPUæœ€é©åŒ–è¨­å®šå®Œäº†:")
        print("   - cuDNN benchmark: æœ‰åŠ¹")
        print("   - ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥: ã‚¯ãƒªã‚¢æ¸ˆã¿")


def finetune_and_evaluate(model_config, args, device):
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡ã‚’å®Ÿè¡Œ"""
    model_name = model_config["name"]
    model_class = model_config["class"]
    model_args = model_config["args"]

    print(f"\n--- {model_name} ã®å‡¦ç†é–‹å§‹ ---")

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = model_class(**model_args)

    if args.finetune:
        print(f"ğŸ“ {model_name} ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")

        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        trainer = SimpleTrainer(
            model,
            device,
            learning_rate=args.learning_rate,
            use_mixed_precision=args.mixed_precision,
            enable_plotting=True,  # å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        )
        model = trainer.quick_finetune(
            tokenizer_name=model_args.get(
                "model_name", "cl-tohoku/bert-base-japanese-v3"
            ),
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            max_length=args.max_length,
            model_name=model_name,  # ãƒ¢ãƒ‡ãƒ«åã‚’æ¸¡ã™
        )
    else:
        print(
            f"âš ï¸  {model_name} ã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ã§è©•ä¾¡ã—ã¾ã™ï¼ˆæ€§èƒ½ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰"
        )

    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = ModelEvaluator(device=device)

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    from data.data_loader import JCommonsenseQALoader

    data_loader_instance = JCommonsenseQALoader(
        tokenizer_name=model_args.get("model_name", "cl-tohoku/bert-base-japanese-v3")
    )
    data_loader = data_loader_instance.create_dataloader(
        split="validation",  # æœ€çµ‚è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆtestãƒ‡ãƒ¼ã‚¿ãŒéå…¬é–‹ã®ãŸã‚validationã‚’ä½¿ç”¨ï¼‰
        batch_size=args.batch_size,
        max_length=args.max_length,
        shuffle=False,
    )

    result = evaluator.evaluate_model(model, data_loader, model_name)

    # ãƒ¡ãƒ¢ãƒªè§£æ”¾ï¼ˆGPUä½¿ç”¨æ™‚é‡è¦ï¼‰
    del model
    if device.type == "cuda":
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    return result


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ“š ModernBERT vs BERT æ€§èƒ½æ¯”è¼ƒè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼‰")
    print("=" * 80)

    args = parse_arguments()

    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    if args.device == "auto":
        device = get_device()
    else:
        device = torch.device(args.device)
        print(f"ğŸ”§ æ‰‹å‹•æŒ‡å®šãƒ‡ãƒã‚¤ã‚¹: {device}")

    # GPUæœ€é©åŒ–è¨­å®š
    if device.type in ["cuda", "mps"]:
        setup_gpu_optimizations(device)

    print(f"\nğŸ”§ è¨­å®š:")
    print(f"  - ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}")
    print(f"  - æœ€å¤§ç³»åˆ—é•·: {args.max_length}")
    print(f"  - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: {'âœ… æœ‰åŠ¹' if args.finetune else 'âŒ ç„¡åŠ¹'}")
    print(f"  - æ··åˆç²¾åº¦å­¦ç¿’: {'âœ… æœ‰åŠ¹' if args.mixed_precision else 'âŒ ç„¡åŠ¹'}")
    if args.finetune:
        print(f"  - ã‚¨ãƒãƒƒã‚¯æ•°: {args.epochs}")
        print(f"  - å­¦ç¿’ç‡: {args.learning_rate}")

    # GPUä½¿ç”¨æ™‚ã®æ¨å¥¨è¨­å®šã‚¢ãƒ‰ãƒã‚¤ã‚¹
    if device.type == "cuda" and not args.mixed_precision:
        print("\nğŸ’¡ GPUæœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ:")
        print("   --mixed_precision ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å­¦ç¿’ã‚’é«˜é€ŸåŒ–ã§ãã¾ã™")

    if device.type == "cuda" and args.batch_size < 16:
        print(f"ğŸ’¡ GPUä½¿ç”¨æ™‚ã¯ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’16ä»¥ä¸Šã«è¨­å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    models_config = []

    if args.model in ["bert", "both"]:
        models_config.append(
            {
                "name": "BERT (æ±åŒ—å¤§v3)",
                "class": BERTModel,
                "args": {
                    "model_name": "cl-tohoku/bert-base-japanese-v3",
                    "num_choices": 5,
                },
            }
        )

    if args.model in ["modern_bert", "both"]:
        models_config.append(
            {
                "name": "ModernBERT (SB-Intuitions)",
                "class": ModernBERTModel,
                "args": {
                    "model_name": "sbintuitions/modernbert-ja-130m",
                    "num_choices": 5,
                },
            }
        )

    results = {}

    # å„ãƒ¢ãƒ‡ãƒ«ã‚’é †æ¬¡å‡¦ç†
    for model_config in models_config:
        try:
            result = finetune_and_evaluate(model_config, args, device)
            results[model_config["name"]] = result
        except Exception as e:
            print(f"âŒ {model_config['name']} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
            if "CUDA out of memory" in str(e):
                print("ğŸ’¡ GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã®è§£æ±ºç­–:")
                print("   - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™: --batch_size 8")
                print("   - ç³»åˆ—é•·ã‚’çŸ­ãã™ã‚‹: --max_length 64")
                print("   - æ··åˆç²¾åº¦ã‚’ä½¿ç”¨: --mixed_precision")
            continue

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“Š **æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼**")
    print("=" * 80)

    if not results:
        print("âŒ è©•ä¾¡çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: JCommonsenseQA (1,119ã‚µãƒ³ãƒ—ãƒ«)")
    print(f"ğŸ¯ ç†è«–çš„ãƒ©ãƒ³ãƒ€ãƒ æ€§èƒ½: 20.0% (5æŠå•é¡Œ)")
    print(f"âš™ï¸  ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: {'å®Ÿè¡Œæ¸ˆã¿' if args.finetune else 'æœªå®Ÿè¡Œ'}")
    print(f"ğŸ”§ å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹: {device}")

    for model_name, result in results.items():
        accuracy = result["accuracy_metrics"]["accuracy"]
        f1_score = result["accuracy_metrics"]["f1_macro"]

        # æ€§èƒ½åˆ¤å®š
        if accuracy > 0.6:
            performance_icon = "ğŸ†"
            performance_text = "å„ªç§€"
        elif accuracy > 0.4:
            performance_icon = "ğŸ¥ˆ"
            performance_text = "è‰¯å¥½"
        elif accuracy > 0.25:
            performance_icon = "ğŸ¥‰"
            performance_text = "æ™®é€š"
        else:
            performance_icon = "âš ï¸"
            performance_text = "è¦æ”¹å–„"

        print(f"\n{performance_icon} **{model_name}** ({performance_text})")
        print(f"   âœ… æ­£è§£ç‡: {accuracy:.1%}")
        print(f"   ğŸ“Š F1ã‚¹ã‚³ã‚¢: {f1_score:.1%}")
        print(f"   ğŸ§® ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {result['model_info']['total_parameters']:,}")

    if not args.finetune:
        print("\nğŸ’¡ **æ”¹å–„ææ¡ˆ**:")
        print(
            "   æ€§èƒ½å‘ä¸Šã®ãŸã‚ã«ã€--finetuneã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è©¦ã—ã¦ãã ã•ã„:"
        )
        if device.type == "cuda":
            print(
                "   python main_with_training.py --finetune --epochs 5 --batch_size 16 --mixed_precision"
            )
        else:
            print(
                "   python main_with_training.py --finetune --epochs 5 --batch_size 4"
            )
    else:
        print("\nğŸ“ˆ **å­¦ç¿’æ›²ç·šã«ã¤ã„ã¦**:")
        print("   - å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ›²ç·š: å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«è¡¨ç¤º")
        print("   - å­¦ç¿’å±¥æ­´ãƒ‡ãƒ¼ã‚¿: results/training_history_*.json")
        print("   - å­¦ç¿’æ›²ç·šç”»åƒ: results/learning_curves_*.png")

    # æœ€çµ‚ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if device.type == "cuda":
        torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
